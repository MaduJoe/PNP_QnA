import os
import pandas as pd
import numpy as np
import pickle
import logging
import re
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import yaml

class HybridSearchEngine:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„: í‚¤ì›Œë“œ ê²€ìƒ‰ + ì„ë² ë”© ê²€ìƒ‰ ê²°í•©
    ì¹´í˜ ê²Œì‹œê¸€ê³¼ Mantis Bugs ë°ì´í„°ë¥¼ ë³„ë„ ìºì‹±ìœ¼ë¡œ ê´€ë¦¬
    """
    
    def __init__(self, config_file="config.yaml"):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”"""
        with open(config_file, "r", encoding="utf-8") as file:
            self.config = yaml.safe_load(file)
        
        # ì„¤ì • ë¡œë“œ
        self.embedding_config = self.config["embedding"]
        self.data_config = self.config["data"]
        
        # ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ì„¤ì • (í‚¤ì›Œë“œì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        self.keyword_weight = 0.6  # í‚¤ì›Œë“œ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ì¦ê°€
        self.embedding_weight = 0.4  # ì„ë² ë”© ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ê°ì†Œ
        
        # ëª¨ë¸ ë° ë°ì´í„° ì´ˆê¸°í™”
        self.model = None
        
        # ì¹´í˜ ê²Œì‹œê¸€ ë°ì´í„°
        self.articles_data = None
        self.articles_embeddings = None
        
        # Mantis Bugs ë°ì´í„° (ë³„ë„ ê´€ë¦¬)
        self.bugs_data = None
        self.bugs_embeddings = None
        
        # í†µí•© ë°ì´í„° (ê²€ìƒ‰ìš©)
        self.combined_data = None
        self.combined_embeddings = None
        
        # TF-IDF ê´€ë ¨
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.embedding_config["cache_dir"], exist_ok=True)
        os.makedirs(self.embedding_config["bugs_cache_dir"], exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    def load_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        try:
            self.logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {self.embedding_config['model_name']}")
            self.model = SentenceTransformer(
                self.embedding_config["model_name"],
                device="cuda" if self._is_cuda_available() else "cpu"
            )
            self.logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def _is_cuda_available(self) -> bool:
        """CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    def load_articles_data(self):
        """ê²Œì‹œê¸€ ë°ì´í„° ë¡œë“œ"""
        try:
            self.logger.info("ê²Œì‹œê¸€ ë°ì´í„° ë¡œë”© ì¤‘...")
            
            # DBS ê²Œì‹œê¸€ ë°ì´í„° ë¡œë“œ
            dbs_path = os.path.join(
                self.data_config["articles_path"], 
                self.data_config["dbs_file"]
            )
            dbs_articles = pd.read_csv(dbs_path, encoding="utf-8")
            dbs_articles["category"] = "DBS"
            
            # Manager ê²Œì‹œê¸€ ë°ì´í„° ë¡œë“œ
            mgr_path = os.path.join(
                self.data_config["articles_path"], 
                self.data_config["mgr_file"]
            )
            mgr_articles = pd.read_csv(mgr_path, encoding="utf-8")
            mgr_articles["category"] = "Manager"

            # sa ê²Œì‹œê¸€ ë°ì´í„° ë¡œë“œ
            sa_path = os.path.join(
                self.data_config["articles_path"], 
                self.data_config["sa_file"]
            )
            sa_articles = pd.read_csv(sa_path, encoding="utf-8")
            sa_articles["category"] = "SA"


            # ë°ì´í„° ê²°í•©
            self.articles_data = pd.concat([dbs_articles, mgr_articles, sa_articles], ignore_index=True)
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            self.articles_data["combined_text"] = self.articles_data.apply(
                self._combine_article_text, axis=1
            )
            
            # í‚¤ì›Œë“œ ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ (ì œëª© ê°•ì¡°)
            self.articles_data["keyword_text"] = self.articles_data.apply(
                self._prepare_keyword_text, axis=1
            )
            
            self.logger.info(f"ì´ {len(self.articles_data)}ê°œì˜ ì¹´í˜ ê²Œì‹œê¸€ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ê²Œì‹œê¸€ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def load_bugs_data(self):
        """Mantis Bugs ë°ì´í„° ë¡œë“œ (ë³„ë„ ìºì‹± ì˜ì—­)"""
        try:
            # Bugs ë°ì´í„° í™œì„±í™” ì—¬ë¶€ í™•ì¸
            if not self.data_config.get("enable_bugs_data", False):
                self.logger.info("Mantis Bugs ë°ì´í„°ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                return
            
            self.logger.info("Mantis Bugs ë°ì´í„° ë¡œë”© ì¤‘...")
            
            # Mantis Bugs ë°ì´í„° ë¡œë“œ
            bugs_path = os.path.join(
                self.data_config["articles_path"], 
                self.data_config["bugs_file"]
            )
            
            if not os.path.exists(bugs_path):
                self.logger.warning(f"Mantis Bugs íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {bugs_path}")
                return
            
            self.bugs_data = pd.read_csv(bugs_path, encoding="utf-8")
            self.bugs_data["category"] = "Bugs"  # Mantis Bugs ì¹´í…Œê³ ë¦¬
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            self.bugs_data["combined_text"] = self.bugs_data.apply(
                self._combine_article_text, axis=1
            )
            
            # í‚¤ì›Œë“œ ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ (ì œëª© ê°•ì¡°)
            self.bugs_data["keyword_text"] = self.bugs_data.apply(
                self._prepare_keyword_text, axis=1
            )
            
            self.logger.info(f"ì´ {len(self.bugs_data)}ê°œì˜ Mantis Bugs ë°ì´í„° ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"Mantis Bugs ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            # Bugs ë°ì´í„° ë¡œë”© ì‹¤íŒ¨ëŠ” ì „ì²´ ì‹œìŠ¤í…œì„ ì¤‘ë‹¨ì‹œí‚¤ì§€ ì•ŠìŒ
            self.bugs_data = None
    
    def _combine_article_text(self, row) -> str:
        """ê²Œì‹œê¸€ì˜ ì œëª©, ë‚´ìš©, ëŒ“ê¸€ì„ ê²°í•© (ì„ë² ë”©ìš©)"""
        title = str(row.get("ì œëª©", "")).strip()
        content = str(row.get("ë‚´ìš©", "")).strip()
        comments = str(row.get("ëŒ“ê¸€", "")).strip()
        
        # í…ìŠ¤íŠ¸ ê²°í•©
        combined = f"{title}\n\n"
        if content and content != "nan":
            combined += f"{content}\n\n"
        if comments and comments != "nan" and comments != "ëŒ“ê¸€ ì—†ìŒ":
            combined += f"{comments}"
        
        return combined.strip()
    
    def _prepare_keyword_text(self, row) -> str:
        """í‚¤ì›Œë“œ ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì œëª©ì— ê°•í•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬)"""
        title = str(row.get("ì œëª©", "")).strip()
        content = str(row.get("ë‚´ìš©", "")).strip()
        comments = str(row.get("ëŒ“ê¸€", "")).strip()
        
        # ì œëª©ì„ 5ë²ˆ ë°˜ë³µí•˜ì—¬ ê°•í•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        keyword_text = f"{title} {title} {title} {title} {title}"
        
        if content and content != "nan":
            keyword_text += f" {content}"
        if comments and comments != "nan" and comments != "ëŒ“ê¸€ ì—†ìŒ":
            keyword_text += f" {comments}"
        
        return keyword_text.strip()
    
    def setup_tfidf(self):
        """TF-IDF ë²¡í„°ë¼ì´ì € ì„¤ì • (í†µí•© ë°ì´í„° ì‚¬ìš©)"""
        if self.combined_data is None:
            self.logger.error("í†µí•© ë°ì´í„°ê°€ ì—†ì–´ TF-IDF ì„¤ì •ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
            
        self.logger.info("TF-IDF ë²¡í„°ë¼ì´ì € ì„¤ì • ì¤‘...")
        
        # í•œêµ­ì–´ì— ìµœì í™”ëœ TF-IDF ì„¤ì •
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=10000,
            ngram_range=(1, 2),  # 1-2gram ì‚¬ìš© (3gramì€ ë„ˆë¬´ ì„¸ë¶„í™”)
            min_df=1,
            max_df=0.8,
            sublinear_tf=True,
            analyzer='word',  # word ë‹¨ìœ„ë¡œ ë³€ê²½ (í•œêµ­ì–´ë„ íš¨ê³¼ì )
            token_pattern=r'\b\w+\b',
        )
        
        # í†µí•© ë°ì´í„°ì˜ í‚¤ì›Œë“œ ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ë¡œ TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        texts = self.combined_data["keyword_text"].tolist()
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
        
        self.logger.info("TF-IDF ì„¤ì • ì™„ë£Œ")
    
    def compute_articles_embeddings(self, force_recompute=False):
        """ì¹´í˜ ê²Œì‹œê¸€ ì„ë² ë”© ê³„ì‚° ë° ìºì‹œ"""
        cache_file = os.path.join(
            self.embedding_config["cache_dir"], 
            "article_embeddings_hybrid.pkl"
        )
        
        # ìºì‹œëœ ì„ë² ë”©ì´ ìˆê³  ì¬ê³„ì‚°ì´ í•„ìš”ì—†ëŠ” ê²½ìš°
        if os.path.exists(cache_file) and not force_recompute:
            try:
                with open(cache_file, "rb") as f:
                    self.articles_embeddings = pickle.load(f)
                self.logger.info("ìºì‹œëœ ì¹´í˜ ê²Œì‹œê¸€ ì„ë² ë”© ë¡œë”© ì™„ë£Œ")
                return
            except Exception as e:
                self.logger.warning(f"ìºì‹œëœ ì¹´í˜ ê²Œì‹œê¸€ ì„ë² ë”© ë¡œë”© ì‹¤íŒ¨: {e}")
        
        # ì„ë² ë”© ê³„ì‚°
        self.logger.info("ì¹´í˜ ê²Œì‹œê¸€ ì„ë² ë”© ê³„ì‚° ì¤‘...")
        if self.model is None:
            self.load_model()
        
        texts = self.articles_data["combined_text"].tolist()
        self.articles_embeddings = self._compute_embeddings_batch(texts, "ì¹´í˜ ê²Œì‹œê¸€")
        
        # ìºì‹œ ì €ì¥
        try:
            with open(cache_file, "wb") as f:
                pickle.dump(self.articles_embeddings, f)
            self.logger.info("ì¹´í˜ ê²Œì‹œê¸€ ì„ë² ë”© ìºì‹œ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"ì¹´í˜ ê²Œì‹œê¸€ ì„ë² ë”© ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def compute_bugs_embeddings(self, force_recompute=False):
        """Mantis Bugs ì„ë² ë”© ê³„ì‚° ë° ë³„ë„ ìºì‹œ"""
        if self.bugs_data is None or len(self.bugs_data) == 0:
            self.logger.info("Mantis Bugs ë°ì´í„°ê°€ ì—†ì–´ ì„ë² ë”©ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        cache_file = os.path.join(
            self.embedding_config["bugs_cache_dir"], 
            "bugs_embeddings_hybrid.pkl"
        )
        
        # ìºì‹œëœ ì„ë² ë”©ì´ ìˆê³  ì¬ê³„ì‚°ì´ í•„ìš”ì—†ëŠ” ê²½ìš°
        if os.path.exists(cache_file) and not force_recompute:
            try:
                with open(cache_file, "rb") as f:
                    self.bugs_embeddings = pickle.load(f)
                self.logger.info("ìºì‹œëœ Mantis Bugs ì„ë² ë”© ë¡œë”© ì™„ë£Œ")
                return
            except Exception as e:
                self.logger.warning(f"ìºì‹œëœ Mantis Bugs ì„ë² ë”© ë¡œë”© ì‹¤íŒ¨: {e}")
        
        # ì„ë² ë”© ê³„ì‚°
        self.logger.info("Mantis Bugs ì„ë² ë”© ê³„ì‚° ì¤‘...")
        if self.model is None:
            self.load_model()
        
        texts = self.bugs_data["combined_text"].tolist()
        self.bugs_embeddings = self._compute_embeddings_batch(texts, "Mantis Bugs")
        
        # ë³„ë„ ìºì‹œì— ì €ì¥
        try:
            with open(cache_file, "wb") as f:
                pickle.dump(self.bugs_embeddings, f)
            self.logger.info("Mantis Bugs ì„ë² ë”© ë³„ë„ ìºì‹œ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"Mantis Bugs ì„ë² ë”© ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _compute_embeddings_batch(self, texts, data_type):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ê³„ì‚°"""
        batch_size = self.embedding_config["batch_size"]
        embeddings_list = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_embeddings = self.model.encode(
                batch_texts,
                convert_to_tensor=False,
                show_progress_bar=True,
                batch_size=min(batch_size, len(batch_texts))
            )
            embeddings_list.append(batch_embeddings)
            
            self.logger.info(f"{data_type} ì§„í–‰ë¥ : {min(i + batch_size, len(texts))}/{len(texts)}")
        
        # ì„ë² ë”© ê²°í•©
        return np.vstack(embeddings_list)
    
    def combine_data_and_embeddings(self):
        """ì¹´í˜ ê²Œì‹œê¸€ê³¼ Mantis Bugs ë°ì´í„°ë¥¼ í†µí•©"""
        self.logger.info("ë°ì´í„° ë° ì„ë² ë”© í†µí•© ì¤‘...")
        
        # ë°ì´í„° í†µí•©
        data_list = [self.articles_data]
        embeddings_list = [self.articles_embeddings]
        
        if self.bugs_data is not None and self.bugs_embeddings is not None:
            data_list.append(self.bugs_data)
            embeddings_list.append(self.bugs_embeddings)
        
        # ë°ì´í„°í”„ë ˆì„ í†µí•©
        self.combined_data = pd.concat(data_list, ignore_index=True)
        
        # ì„ë² ë”© í†µí•©
        self.combined_embeddings = np.vstack(embeddings_list)
        
        self.logger.info(f"í†µí•© ì™„ë£Œ: ì´ {len(self.combined_data)}ê°œ ë°ì´í„° (ì¹´í˜: {len(self.articles_data)}, Bugs: {len(self.bugs_data) if self.bugs_data is not None else 0})")
    
    def compute_embeddings(self, force_recompute=False):
        """ì „ì²´ ì„ë² ë”© ê³„ì‚° (í˜¸í™˜ì„± ìœ ì§€)"""
        self.compute_articles_embeddings(force_recompute)
        self.compute_bugs_embeddings(force_recompute)
        self.combine_data_and_embeddings()
    
    def keyword_search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰"""
        # ì¿¼ë¦¬ë¥¼ TF-IDF ë²¡í„°ë¡œ ë³€í™˜
        query_vector = self.tfidf_vectorizer.transform([query])
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(query_vector, self.tfidf_matrix)[0]
        
        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤ì™€ ì ìˆ˜ ë°˜í™˜
        top_indices = np.argsort(similarities)[::-1][:top_k]
        results = [(idx, similarities[idx]) for idx in top_indices if similarities[idx] > 0]
        
        return results
    
    def embedding_search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        """ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ (í†µí•© ë°ì´í„° ì‚¬ìš©)"""
        if self.model is None:
            self.load_model()
        
        if self.combined_embeddings is None:
            self.logger.error("í†µí•© ì„ë² ë”©ì´ ì—†ì–´ ì„ë² ë”© ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”© ê³„ì‚°
        query_embedding = self.model.encode([query], convert_to_tensor=False)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(query_embedding, self.combined_embeddings)[0]
        
        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤ì™€ ì ìˆ˜ ë°˜í™˜
        top_indices = np.argsort(similarities)[::-1][:top_k]
        results = [(idx, similarities[idx]) for idx in top_indices]
        
        return results
    
    def exact_keyword_bonus(self, query: str, article_title: str, article_text: str) -> float:
        """ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ì— ëŒ€í•œ ë³´ë„ˆìŠ¤ ì ìˆ˜"""
        query_words = set(re.findall(r'\w+', query.lower()))
        title_words = set(re.findall(r'\w+', article_title.lower()))
        article_words = set(re.findall(r'\w+', article_text.lower()))
        
        if not query_words:
            return 0.0
        
        # ì œëª©ì—ì„œ ë§¤ì¹­ë˜ëŠ” ê²½ìš° ë†’ì€ ë³´ë„ˆìŠ¤
        title_match_ratio = len(query_words.intersection(title_words)) / len(query_words)
        
        # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë§¤ì¹­ë˜ëŠ” ê²½ìš° ì¼ë°˜ ë³´ë„ˆìŠ¤
        text_match_ratio = len(query_words.intersection(article_words)) / len(query_words)
        
        # ì œëª© ë§¤ì¹­ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        total_bonus = title_match_ratio * 0.5 + text_match_ratio * 0.2
        
        return min(total_bonus, 0.7)  # ìµœëŒ€ 0.7ì  ë³´ë„ˆìŠ¤
    
    def hybrid_search(self, query: str, top_k: int = None) -> List[Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: í‚¤ì›Œë“œ + ì„ë² ë”© ê²°í•©"""
        if top_k is None:
            top_k = self.embedding_config["top_k"]
        
        try:
            # 1. í‚¤ì›Œë“œ ê²€ìƒ‰ ìˆ˜í–‰
            keyword_results = self.keyword_search(query, top_k=20)
            
            # 2. ì„ë² ë”© ê²€ìƒ‰ ìˆ˜í–‰
            embedding_results = self.embedding_search(query, top_k=20)
            
            # 3. ì ìˆ˜ ì •ê·œí™” ë° ê²°í•©
            combined_scores = {}
            
            # í‚¤ì›Œë“œ ê²€ìƒ‰ ì ìˆ˜ ì¶”ê°€
            if keyword_results:
                max_keyword_score = max([score for _, score in keyword_results])
                for idx, score in keyword_results:
                    normalized_score = score / max_keyword_score if max_keyword_score > 0 else 0
                    combined_scores[idx] = combined_scores.get(idx, 0) + normalized_score * self.keyword_weight
            
            # ì„ë² ë”© ê²€ìƒ‰ ì ìˆ˜ ì¶”ê°€
            if embedding_results:
                max_embedding_score = max([score for _, score in embedding_results])
                for idx, score in embedding_results:
                    normalized_score = score / max_embedding_score if max_embedding_score > 0 else 0
                    combined_scores[idx] = combined_scores.get(idx, 0) + normalized_score * self.embedding_weight
            
            # 4. ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤ ì¶”ê°€
            for idx in combined_scores:
                article = self.combined_data.iloc[idx]
                bonus = self.exact_keyword_bonus(
                    query, 
                    article["ì œëª©"], 
                    article["combined_text"]
                )
                combined_scores[idx] += bonus
            
            # 5. ìƒìœ„ kê°œ ì„ íƒ
            sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
            
            # 6. ê²°ê³¼ í¬ë§·íŒ…
            results = []
            for idx, combined_score in sorted_results:
                article = self.combined_data.iloc[idx]
                
                # ê°œë³„ ì ìˆ˜ ê³„ì‚° (ë””ë²„ê¹…ìš©)
                keyword_score = 0
                embedding_score = 0
                
                for k_idx, k_score in keyword_results:
                    if k_idx == idx:
                        keyword_score = k_score
                        break
                
                for e_idx, e_score in embedding_results:
                    if e_idx == idx:
                        embedding_score = e_score
                        break
                
                result = {
                    "title": article["ì œëª©"],
                    "content": article["ë‚´ìš©"][:200] + "..." if len(str(article["ë‚´ìš©"])) > 200 else article["ë‚´ìš©"],
                    "url": article["URL"],
                    "category": article["category"],
                    "combined_score": float(combined_score),
                    "keyword_score": float(keyword_score),
                    "embedding_score": float(embedding_score),
                    "ê²Œì‹œê¸€ID": article["ê²Œì‹œê¸€ID"]
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            self.logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise
    
    def hybrid_search_by_category(self, query: str, product_top_k: int = 3, bugs_top_k: int = 3) -> Dict[str, List[Dict]]:
        """ì¹´í…Œê³ ë¦¬ë³„ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: Product QnAì™€ Bugs QnA ë¶„ë¦¬"""
        try:
            # 1. í‚¤ì›Œë“œ ê²€ìƒ‰ ìˆ˜í–‰ (ì „ì²´)
            keyword_results = self.keyword_search(query, top_k=50)
            
            # 2. ì„ë² ë”© ê²€ìƒ‰ ìˆ˜í–‰ (ì „ì²´)
            embedding_results = self.embedding_search(query, top_k=50)
            
            # 3. ì ìˆ˜ ì •ê·œí™” ë° ê²°í•©
            combined_scores = {}
            
            # í‚¤ì›Œë“œ ê²€ìƒ‰ ì ìˆ˜ ì¶”ê°€
            if keyword_results:
                max_keyword_score = max([score for _, score in keyword_results])
                for idx, score in keyword_results:
                    normalized_score = score / max_keyword_score if max_keyword_score > 0 else 0
                    combined_scores[idx] = combined_scores.get(idx, 0) + normalized_score * self.keyword_weight
            
            # ì„ë² ë”© ê²€ìƒ‰ ì ìˆ˜ ì¶”ê°€
            if embedding_results:
                max_embedding_score = max([score for _, score in embedding_results])
                for idx, score in embedding_results:
                    normalized_score = score / max_embedding_score if max_embedding_score > 0 else 0
                    combined_scores[idx] = combined_scores.get(idx, 0) + normalized_score * self.embedding_weight
            
            # 4. ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤ ì¶”ê°€
            for idx in combined_scores:
                article = self.combined_data.iloc[idx]
                bonus = self.exact_keyword_bonus(
                    query, 
                    article["ì œëª©"], 
                    article["combined_text"]
                )
                combined_scores[idx] += bonus
            
            # 5. URL ê¸°ì¤€ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¦¬
            product_scores = {}
            bugs_scores = {}
            
            for idx, combined_score in combined_scores.items():
                article = self.combined_data.iloc[idx]
                url = str(article["URL"])
                
                # Product QnA: ì¹´í˜ URL (cafe.naver.com)
                if "cafe.naver.com" in url:
                    product_scores[idx] = combined_score
                # Bugs QnA: ë²„ê·¸ ì¶”ì  URL (bugs.pnpsecure.com)
                elif "bugs.pnpsecure.com" in url:
                    bugs_scores[idx] = combined_score
            
            # 6. ê° ì¹´í…Œê³ ë¦¬ë³„ ìƒìœ„ kê°œ ì„ íƒ
            product_results = sorted(product_scores.items(), key=lambda x: x[1], reverse=True)[:product_top_k]
            bugs_results = sorted(bugs_scores.items(), key=lambda x: x[1], reverse=True)[:bugs_top_k]
            
            # 7. ê²°ê³¼ í¬ë§·íŒ…
            def format_category_results(category_results, category_name):
                results = []
                for idx, combined_score in category_results:
                    article = self.combined_data.iloc[idx]
                    
                    # ê°œë³„ ì ìˆ˜ ê³„ì‚°
                    keyword_score = 0
                    embedding_score = 0
                    
                    for k_idx, k_score in keyword_results:
                        if k_idx == idx:
                            keyword_score = k_score
                            break
                    
                    for e_idx, e_score in embedding_results:
                        if e_idx == idx:
                            embedding_score = e_score
                            break
                    
                    result = {
                        "title": article["ì œëª©"],
                        "content": article["ë‚´ìš©"][:200] + "..." if len(str(article["ë‚´ìš©"])) > 200 else article["ë‚´ìš©"],
                        "url": article["URL"],
                        "category": article["category"],
                        "combined_score": float(combined_score),
                        "keyword_score": float(keyword_score),
                        "embedding_score": float(embedding_score),
                        "ê²Œì‹œê¸€ID": article["ê²Œì‹œê¸€ID"]
                    }
                    results.append(result)
                return results
            
            return {
                "product_qna": format_category_results(product_results, "Product QnA"),
                "bugs_qna": format_category_results(bugs_results, "Bugs QnA")
            }
            
        except Exception as e:
            self.logger.error(f"ì¹´í…Œê³ ë¦¬ë³„ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            raise
    
    def initialize(self):
        """ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” (ì¹´í˜ ê²Œì‹œê¸€ + Mantis Bugs í†µí•©)"""
        self.logger.info("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ì‹œì‘...")
        
        # ëª¨ë¸ ë¡œë“œ
        self.load_model()
        
        # ë°ì´í„° ë¡œë“œ
        self.load_articles_data()    # ì¹´í˜ ê²Œì‹œê¸€ ë¡œë“œ
        self.load_bugs_data()        # Mantis Bugs ë¡œë“œ
        
        # ì„ë² ë”© ê³„ì‚° (ë³„ë„ ìºì‹±)
        self.compute_articles_embeddings()  # ì¹´í˜ ê²Œì‹œê¸€ ì„ë² ë”©
        self.compute_bugs_embeddings()      # Mantis Bugs ì„ë² ë”©
        
        # ë°ì´í„° ë° ì„ë² ë”© í†µí•©
        self.combine_data_and_embeddings()
        
        # TF-IDF ì„¤ì • (í†µí•© ë°ì´í„° ê¸°ë°˜)
        self.setup_tfidf()
        
        self.logger.info("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def format_search_results(self, results: List[Dict], query: str) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‚¬ìš©ìì—ê²Œ í‘œì‹œí•  í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if not results:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        response = f"ğŸ” '{query}' í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼\n\n"
        
        for i, result in enumerate(results, 1):
            response += f"ğŸ“Œ {i}. {result['title']}\n"
            response += f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {result['category']}\n"
            response += f"ğŸ¯ ì¢…í•©ì ìˆ˜: {result['combined_score']:.3f}\n"
            response += f"ğŸ”— {result['url']}\n\n"
        
        return response

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    search_engine = HybridSearchEngine()
    search_engine.initialize()
    
    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    test_query = "ë§¤ë‹ˆì € ë¡œê·¸ì¸ ì˜¤ë¥˜"
    results = search_engine.hybrid_search(test_query)
    print(search_engine.format_search_results(results, test_query)) 
